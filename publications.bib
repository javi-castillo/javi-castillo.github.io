
@article{castillo-navarro_semi-supervised_2022,
	title = {Semi-supervised semantic segmentation in Earth Observation: the {MiniFrance} suite, dataset analysis and multi-task network study},
	volume = {111},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-020-05943-y},
	doi = {10.1007/s10994-020-05943-y},
	shorttitle = {Semi-supervised semantic segmentation in Earth Observation},
	abstract = {The development of semi-supervised learning techniques is essential to enhance the generalization capacities of machine learning algorithms. Indeed, raw image data are abundant while labels are scarce, therefore it is crucial to leverage unlabeled inputs to build better models. The availability of large databases have been key for the development of learning algorithms with high level performance. Despite the major role of machine learning in Earth Observation to derive products such as land cover maps, datasets in the field are still limited, either because of modest surface coverage, lack of variety of scenes or restricted classes to identify. We introduce a novel large-scale dataset for semi-supervised semantic segmentation in Earth Observation, the {MiniFrance} suite. {MiniFrance} has several unprecedented properties: it is large-scale, containing over 2000 very high resolution aerial images, accounting for more than 200 billions samples (pixels); it is varied, covering 16 conurbations in France, with various climates, different landscapes, and urban as well as countryside scenes; and it is challenging, considering land use classes with high-level semantics. Nevertheless, the most distinctive quality of {MiniFrance} is being the only dataset in the field especially designed for semi-supervised learning: it contains labeled and unlabeled images in its training partition, which reproduces a life-like scenario. Along with this dataset, we present tools for data representativeness analysis in terms of appearance similarity and a thorough study of {MiniFrance} data, demonstrating that it is suitable for learning and generalizes well in a semi-supervised setting. Finally, we present semi-supervised deep architectures based on multi-task learning and the first experiments on {MiniFrance}. These results will serve as baselines for future work on semi-supervised learning over the {MiniFrance} dataset. The Minifrance suite and related semi-supervised networks will be publicly available to promote semi-supervised works in Earth Observation.},
	pages = {3125--3160},
	number = {9},
	journaltitle = {Machine Learning},
	shortjournal = {Mach Learn},
	author = {Castillo-Navarro, Javiera and Le Saux, Bertrand and Boulch, Alexandre and Audebert, Nicolas and Lefèvre, Sébastien},
	urldate = {2024-12-29},
	date = {2022-09-01},
	langid = {english},
	keywords = {Artificial Intelligence, Earth Observation, Land use mapping, Large-scale dataset, Semantic segmentation, Semi-supervised learning},
	file = {Full Text PDF:/Users/javiera/Zotero/storage/GM7UG6CZ/Castillo-Navarro et al. - 2022 - Semi-supervised semantic segmentation in Earth Observation the MiniFrance suite, dataset analysis a.pdf:application/pdf},
}

@article{castillo-navarro_energy-based_2022,
	title = {Energy-Based Models in Earth Observation: From Generation to Semisupervised Learning},
	volume = {60},
	issn = {1558-0644},
	url = {https://ieeexplore.ieee.org/document/9606737},
	doi = {10.1109/TGRS.2021.3126428},
	shorttitle = {Energy-Based Models in Earth Observation},
	abstract = {Deep learning, together with the availability of large amounts of data, has transformed the way we process Earth observation ({EO}) tasks, such as land cover mapping or image registration. Yet, today, new models are needed to push further the revolution and enable new possibilities. This work focuses on a recent framework for generative modeling and explores its applicability to the {EO} images. The framework learns an energy-based model ({EBM}) to estimate the underlying joint distribution of the data and the categories, obtaining a neural network that is able to classify and synthesize images. On these two tasks, we show that {EBMs} reach comparable or better performances than convolutional networks on various public {EO} datasets and that they are naturally adapted to semisupervised settings, with very few labeled data. Moreover, models of this kind allow us to address high-potential applications, such as out-of-distribution analysis and land cover mapping with confidence estimation.},
	pages = {1--11},
	journaltitle = {{IEEE} Transactions on Geoscience and Remote Sensing},
	author = {Castillo-Navarro, Javiera and Le Saux, Bertrand and Boulch, Alexandre and Lefèvre, Sébastien},
	urldate = {2024-12-29},
	date = {2022},
	note = {Conference Name: {IEEE} Transactions on Geoscience and Remote Sensing},
	keywords = {Adaptation models, Biological system modeling, Data models, Deep learning, Earth, energy-based models ({EBMs}), generative models, semisupervised learning, Semisupervised learning, Training},
	file = {IEEE Xplore Abstract Record:/Users/javiera/Zotero/storage/HGZ79CK4/9606737.html:text/html;Version soumise:/Users/javiera/Zotero/storage/SCRX9CI9/Castillo-Navarro et al. - 2022 - Energy-Based Models in Earth Observation From Generation to Semisupervised Learning.pdf:application/pdf},
}

@article{hansch_2022_2022,
	title = {The 2022 {IEEE} {GRSS} Data Fusion Contest: Semisupervised Learning [Technical Committees]},
	volume = {10},
	issn = {2168-6831},
	url = {https://ieeexplore.ieee.org/document/9764835/?arnumber=9764835},
	doi = {10.1109/MGRS.2022.3144291},
	shorttitle = {The 2022 {IEEE} {GRSS} Data Fusion Contest},
	abstract = {Data availability plays a central role in any machine learning setup, especially since the rise of deep learning. Although input data are often available in abundance, reference data used to train and evaluate corresponding approaches are usually scarce due to the high cost of obtaining them. Although this is not limited to remote sensing, it is of particular importance in Earth-observation applications. Semisupervised learning is one approach to mitigate this challenge and leverage the large amount of available input data while relying only on a small, annotated training set.},
	pages = {334--337},
	number = {1},
	journaltitle = {{IEEE} Geoscience and Remote Sensing Magazine},
	author = {Hänsch, Ronny and Persello, Claudio and Vivone, Gemine and Navarro, Javiera Castillo and Boulch, Alexandre and Lefevre, Sebastien and Saux, Bertrand},
	urldate = {2024-12-29},
	date = {2022-03},
	note = {Conference Name: {IEEE} Geoscience and Remote Sensing Magazine},
	file = {Full Text PDF:/Users/javiera/Zotero/storage/BNKBEQZI/Hänsch et al. - 2022 - The 2022 IEEE GRSS Data Fusion Contest Semisupervised Learning [Technical Committees].pdf:application/pdf;IEEE Xplore Abstract Record:/Users/javiera/Zotero/storage/XEVMXP6S/9764835.html:text/html},
}

@inproceedings{castillo-navarro_what_2019,
	title = {What Data are needed for Semantic Segmentation in Earth Observation?},
	url = {https://ieeexplore.ieee.org/document/8809071},
	doi = {10.1109/JURSE.2019.8809071},
	abstract = {This paper explores different aspects of semantic segmentation of remote sensing data using deep neural networks. Learning with deep neural networks was revolutionized by the creation of {ImageNet}. Remote sensing benefited of these new techniques, however Earth Observation ({EO}) datasets remain small in comparison. In this work, we investigate how we can progress towards the {ImageNet} of remote sensing. In particular, two questions are addressed in this paper. First, how robust are existing supervised learning strategies with respect to data volume? Second, which properties are expected from a large-scale {EO} dataset? The main contributions of this work are: (i) a strong robustness analysis of existing supervised learning strategies with respect to remote sensing data, (ii) the introduction of a new, large-scale dataset named {MiniFrance}.},
	eventtitle = {2019 Joint Urban Remote Sensing Event ({JURSE})},
	pages = {1--4},
	booktitle = {2019 Joint Urban Remote Sensing Event ({JURSE})},
	author = {Castillo-Navarro, J. and Audebert, N. and Boulch, A. and Le Saux, B. and Lefèvre, S.},
	urldate = {2024-12-29},
	date = {2019-05},
	note = {{ISSN}: 2642-9535},
	keywords = {Deep Learning, Earth, Image segmentation, Land Use/Land Cover Mapping, Remote sensing, Semantic Segmentation, Semantics, Supervised learning, Supervised Learning, Training, Urban areas},
	file = {IEEE Xplore Abstract Record:/Users/javiera/Zotero/storage/QVQSRKVI/8809071.html:text/html;Version soumise:/Users/javiera/Zotero/storage/5CAA5JF3/Castillo-Navarro et al. - 2019 - What Data are needed for Semantic Segmentation in Earth Observation.pdf:application/pdf},
}

@inproceedings{castillo-navarro_auxiliary_2020,
	location = {{GHENT}, Belgium},
	title = {On Auxiliary Losses for Semi-Supervised Semantic Segmentation},
	url = {https://hal.science/hal-03206301},
	abstract = {The development of semi-supervised learning methods is essential to Earth Observation applications. Indeed, labeled remote sensing data are scarce and likely insufficient to train fully supervised models with good generalization capacities. Conversely, raw data are abundant and therefore it is crucial to leverage unlabeled inputs to build better deep learning models. This work addresses the problem of semisupervised semantic segmentation from a multi-task learning perspective. In this context, we explore several auxiliary tasks (reconstruction, unsupervised segmentation or self-supervision), and corresponding unsupervised losses, to perform along with semantic segmentation. Our experiments show the potential of semi-supervised learning approaches in a lifelike scenario, outperforming a classical supervised setting.},
	booktitle = {{ECML} {PKDD} 2020: European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases},
	author = {Castillo-Navarro, Javiera and Le Saux, Bertrand and Boulch, Alexandre and Lefèvre, Sébastien},
	urldate = {2024-12-29},
	date = {2020-09},
	keywords = {Multitask Learning, Semantic Segmentation, Semi-Supervised Learning},
	file = {HAL PDF Full Text:/Users/javiera/Zotero/storage/7L58RNDG/Castillo-Navarro et al. - 2020 - On Auxiliary Losses for Semi-Supervised Semantic Segmentation.pdf:application/pdf},
}

@article{hansch_report_2022,
	title = {Report on the 2022 {IEEE} Geoscience and Remote Sensing Society Data Fusion Contest: Semisupervised Learning [Technical Committees]},
	volume = {10},
	issn = {2168-6831},
	url = {https://ieeexplore.ieee.org/document/9956742},
	doi = {10.1109/MGRS.2022.3219935},
	shorttitle = {Report on the 2022 {IEEE} Geoscience and Remote Sensing Society Data Fusion Contest},
	abstract = {The Image Analysis and Data Fusion ({IADF}) Technical Committee ({TC}) of the {IEEE} Geoscience and Remote Sensing Society ({GRSS}) has been organizing the annual Data Fusion Contest ({DFC}) since 2006. The contest promotes the development of methods for extracting geospatial information from large-scale, multisensor, multimodal, and multitemporal data. It aims to propose new problem settings that are challenging to address with existing techniques and to establish new benchmarks for scientific challenges in remote sensing image analysis [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19].},
	pages = {270--273},
	number = {4},
	journaltitle = {{IEEE} Geoscience and Remote Sensing Magazine},
	author = {Hänsch, Ronny and Persello, Claudio and Vivone, Gemine and Navarro, Javiera Castillo and Boulch, Alexandre and Lefevre, Sebastien and Saux, Bertrand Le},
	urldate = {2024-12-29},
	date = {2022-12},
	note = {Conference Name: {IEEE} Geoscience and Remote Sensing Magazine},
	file = {Full Text PDF:/Users/javiera/Zotero/storage/HNZGSVNN/Hänsch et al. - 2022 - Report on the 2022 IEEE Geoscience and Remote Sensing Society Data Fusion Contest Semisupervised Le.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/javiera/Zotero/storage/D7K3SKNH/9956742.html:text/html},
}

@inproceedings{zermatten_text_2023,
	title = {Text as a Richer Source of Supervision in Semantic Segmentation Tasks},
	url = {https://infoscience.epfl.ch/handle/20.500.14299/203271},
	doi = {10.1109/IGARSS52108.2023.10282398},
	abstract = {This paper introduces {TACOSS} a text-image alignment approach that allows explainable land cover semantic segmentation by directly integrating semantic concepts encoded from texts. {TACOSS} combines convolutional neural networks for visual feature extraction with semantic embeddings provided by a language model. By leveraging contrastive learning approaches, we learn an alignment between the visual and the (fixed) textual representations. In addition to producing standard semantic segmentation outputs, our model enables interactive queries with {RS} images using natural language prompts. The experimental results obtained on 50cm resolution aerial data from Switzerland show that {TACOSS} performs similarly to a standard semantic segmentation model while allowing the flexible usage of in- and out-of-vocabulary terms for the interactions with the image.},
	eventtitle = {International Geoscience and Remote Sensing Symposium ({IGARSS})},
	pages = {2219--2222},
	booktitle = {{IGARSS} 2023 - 2023 {IEEE} International Geoscience and Remote Sensing Symposium. Proceedings},
	publisher = {The Institute of Electrical and Electronics Engineers, Inc},
	author = {Zermatten, Valérie and Castillo Navarro, Javiera and Hughes, Lloyd and Kellenberger, Tobias and Tuia, Devis},
	urldate = {2024-12-29},
	date = {2023},
	langid = {french},
	file = {Full Text PDF:/Users/javiera/Zotero/storage/R3B6BD8S/Zermatten et al. - 2023 - Text as a Richer Source of Supervision in Semantic Segmentation Tasks.pdf:application/pdf},
}

@article{castillo_navarro_reseaux_nodate,
	title = {Réseaux de neurones semi-supervisés pour la segmentation sémantique en télédétection},
	abstract = {This work explores different aspects of semantic segmentation of remote sensing data using deep neural networks.The availability of large databases of fully annotated data is the basis for good performances of current neural networks. Although images on remote sensing are abundant, annotations are very rare or difﬁcult to produce. In this context, two questions arise: ﬁrst, how robust are existing supervised learning strategies with respect to data? Second, is it possible to improve performance of current methods by using non annotated data? Our main contributions are: (i) a strong robustness analysis of existing supervised learning strategies with respect to remote sensing data, (ii) the introduction of a semi-supervised architecture, capable of learning from annotated and non annotated images simultaneously.},
	author = {Castillo Navarro, Javiera and Le Saux, Bertrand and Boulch, Alexandre and Lefèvre, Sébastien},
	langid = {french},
	file = {PDF:/Users/javiera/Zotero/storage/PJK2EPPQ/Navarro et al. - Réseaux de neurones semi-supervisés pour la segmentation sémantique en télédétection.pdf:application/pdf},
}

@misc{mi_knowledge-aware_2024,
	title = {Knowledge-aware Text-Image Retrieval for Remote Sensing Images},
	url = {http://arxiv.org/abs/2405.03373},
	doi = {10.48550/arXiv.2405.03373},
	abstract = {Image-based retrieval in large Earth observation archives is challenging because one needs to navigate across thousands of candidate matches only with the query image as a guide. By using text as information supporting the visual query, the retrieval system gains in usability, but at the same time faces difficulties due to the diversity of visual signals that cannot be summarized by a short caption only. For this reason, as a matching-based task, cross-modal text-image retrieval often suffers from information asymmetry between texts and images. To address this challenge, we propose a Knowledge-aware Text-Image Retrieval ({KTIR}) method for remote sensing images. By mining relevant information from an external knowledge graph, {KTIR} enriches the text scope available in the search query and alleviates the information gaps between texts and images for better matching. Moreover, by integrating domain-specific knowledge, {KTIR} also enhances the adaptation of pre-trained vision-language models to remote sensing applications. Experimental results on three commonly used remote sensing text-image retrieval benchmarks show that the proposed knowledge-aware method leads to varied and consistent retrievals, outperforming state-of-the-art retrieval methods.},
	number = {{arXiv}:2405.03373},
	publisher = {{arXiv}},
	author = {Mi, Li and Dai, Xianjie and Castillo-Navarro, Javiera and Tuia, Devis},
	urldate = {2024-12-29},
	date = {2024-10-25},
	eprinttype = {arxiv},
	eprint = {2405.03373 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/javiera/Zotero/storage/7XPBNSYF/Mi et al. - 2024 - Knowledge-aware Text-Image Retrieval for Remote Sensing Images.pdf:application/pdf;Snapshot:/Users/javiera/Zotero/storage/VAVEV3WK/2405.html:text/html},
}

@article{mi_convqg_2024,
	title = {{ConVQG}: Contrastive Visual Question Generation with Multimodal Guidance},
	volume = {38},
	rights = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/28216},
	doi = {10.1609/aaai.v38i5.28216},
	shorttitle = {{ConVQG}},
	abstract = {Asking questions about visual environments is a crucial way for intelligent agents to understand rich multi-faceted scenes, raising the importance of Visual Question Generation ({VQG}) systems. Apart from being grounded to the image, existing {VQG} systems can use textual constraints, such as expected answers or knowledge triplets, to generate focused questions. These constraints allow {VQG} systems to specify the question content or leverage external commonsense knowledge that can not be obtained from the image content only. However, generating focused questions using textual constraints while enforcing a high relevance to the image content remains a challenge, as {VQG} systems often ignore one or both forms of grounding. In this work, we propose Contrastive Visual Question Generation ({ConVQG}), a method using a dual contrastive objective to discriminate questions generated using both modalities from those based on a single one. Experiments on both knowledge-aware and standard {VQG} benchmarks demonstrate that {ConVQG} outperforms the state-of-the-art methods and generates image-grounded, text-guided, and knowledge-rich questions. Our human evaluation results also show preference for {ConVQG} questions compared to non-contrastive baselines.},
	pages = {4207--4215},
	number = {5},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Mi, Li and Montariol, Syrielle and Navarro, Javiera Castillo and Dai, Xianjie and Bosselut, Antoine and Tuia, Devis},
	urldate = {2024-12-29},
	date = {2024-03-24},
	langid = {english},
	note = {Number: 5},
	keywords = {{CV}: Language and Vision},
	file = {Full Text PDF:/Users/javiera/Zotero/storage/P9LK6FSQ/Mi et al. - 2024 - ConVQG Contrastive Visual Question Generation with Multimodal Guidance.pdf:application/pdf},
}

@inproceedings{tartini-chappuis_multi-task_2023,
	title = {Multi-task prompt-{RSVQA} to explicitly count objects on aerial images},
	url = {https://infoscience.epfl.ch/handle/20.500.14299/205803},
	abstract = {Introduced to enable a wider use of Earth Observation images using natural language, Remote Sensing Visual Question Answering ({RSVQA}) remains a challenging task, in particular for questions related to counting. To address this specific challenge, we propose a modular Multi-task prompt-{RSVQA} model based on object detection and question answering modules. By creating a semantic bottleneck describing the image and providing a visual answer, our model allows users to assess the visual grounding of the answer and better interpret the prediction. A set of ablation studies are designed to consider the contributions of different modules and evaluation metrics are discussed for a finer-grained assessment. Experiments demonstrate competitive results against literature baselines and a zero-shot {VQA} model. In particular, our proposed model predicts answers for numerical Counting questions that are consistently closer in distance to the ground truth.},
	eventtitle = {British Machine Vision Conference ({BMVC}) workshops},
	author = {Tartini-Chappuis, Christel and Sertic, Charlotte and Santacroce, Nicolas and Castillo Navarro, Javiera and Lobry, Sylvain and Le Saux, Bertrand and Tuia, Devis},
	urldate = {2024-12-29},
	date = {2023-09-01},
	langid = {french},
	file = {Full Text PDF:/Users/javiera/Zotero/storage/82HRFBR2/Tartini-Chappuis et al. - 2023 - Multi-task prompt-RSVQA to explicitly count objects on aerial images.pdf:application/pdf},
}

@inproceedings{castillo-navarro_classification_2021,
	title = {Classification and Generation of Earth Observation Images Using a Joint Energy-Based Model},
	url = {https://ieeexplore.ieee.org/document/9553440},
	doi = {10.1109/IGARSS47720.2021.9553440},
	abstract = {Deep learning has changed unbelievably the processing of Earth Observation tasks such as land cover mapping or image registration. Yet, today new models are needed to push further the revolution and enable new possibilities. We propose a new framework for generative modelling of Earth Observation images. It learns an energy-based model to estimate the underlying distribution of the data while jointly training a deep neural network for classification. On the varied image types of the {EuroSAT} benchmark, we show this model obtains classification results on par with state-of-the-art and moreover allows us to tackle a wide range of high-potential applications: image synthesis, out-of-distribution testing for domain adaptation, and image completion or denoising.},
	eventtitle = {2021 {IEEE} International Geoscience and Remote Sensing Symposium {IGARSS}},
	pages = {2098--2101},
	booktitle = {2021 {IEEE} International Geoscience and Remote Sensing Symposium {IGARSS}},
	author = {Castillo-Navarro, Javiera and Le Saux, Bertrand and Boulch, Alexandre and Lefèvre, Sébastien},
	urldate = {2024-12-29},
	date = {2021-07},
	note = {{ISSN}: 2153-7003},
	keywords = {Adaptation models, Deep learning, Deep Learning, Earth, Earth Observation, Energy-based Models, Generative Models, Image registration, Image synthesis, Noise reduction, Training},
	file = {IEEE Xplore Abstract Record:/Users/javiera/Zotero/storage/XWUTUPN9/9553440.html:text/html;Version soumise:/Users/javiera/Zotero/storage/65FL9NXA/Castillo-Navarro et al. - 2021 - Classification and Generation of Earth Observation Images Using a Joint Energy-Based Model.pdf:application/pdf},
}

@inproceedings{li_knowledge-aware_2024,
	title = {Knowledge-Aware Visual Question Generation for Remote Sensing Images},
	url = {https://ieeexplore.ieee.org/document/10642766},
	doi = {10.1109/IGARSS53475.2024.10642766},
	abstract = {With the rapid development of remote sensing image archives, asking questions about images has become an effective way of gathering specific information or performing image retrieval. However, automatically generated image-based questions tend to be simplistic and template-based, which hinders the real deployment of question answering or visual dialogue systems. To enrich and diversify the questions, we propose a knowledge-aware remote sensing visual question generation model, {KRSVQG}, that incorporates external knowledge related to the image content to improve the quality and contextual understanding of the generated questions. The model takes an image and a related knowledge triplet from external knowledge sources as inputs and leverages image captioning as an intermediary representation to enhance the image grounding of the generated questions. To assess the performance of {KRSVQG}, we utilized two datasets that we manually annotated: {NWPU}-300 and {TextRS}-300. Results on these two datasets demonstrate that {KRSVQG} outperforms existing methods and leads to knowledge-enriched questions, grounded in both image and domain knowledge.},
	eventtitle = {{IGARSS} 2024 - 2024 {IEEE} International Geoscience and Remote Sensing Symposium},
	pages = {498--502},
	booktitle = {{IGARSS} 2024 - 2024 {IEEE} International Geoscience and Remote Sensing Symposium},
	author = {Li, Siran and Mi, Li and Castillo-Navarro, Javiera and Tuia, Devis},
	urldate = {2024-12-29},
	date = {2024-07},
	note = {{ISSN}: 2153-7003},
	keywords = {Grounding, Image retrieval, Knowledge-aware Vision-Language Models, Question answering (information retrieval), Question generation, Remote Sensing, Robustness, Sensors, Visual Question Generation, Visualization},
	file = {IEEE Xplore Abstract Record:/Users/javiera/Zotero/storage/FTTGCG33/10642766.html:text/html},
}

@inproceedings{mi_congeo_2025,
	location = {Cham},
	title = {{ConGeo}: Robust Cross-View Geo-Localization Across Ground View Variations},
	isbn = {978-3-031-72630-9},
	doi = {10.1007/978-3-031-72630-9_13},
	shorttitle = {{ConGeo}},
	abstract = {Cross-view geo-localization aims at localizing a ground-level query image by matching it to its corresponding geo-referenced aerial view. In real-world scenarios, the task requires accommodating diverse ground images captured by users with varying orientations and reduced field of views ({FoVs}). However, existing learning pipelines are orientation-specific or {FoV}-specific, demanding separate model training for different ground view variations. Such models heavily depend on the North-aligned spatial correspondence and predefined {FoVs} in the training data, compromising their robustness across different settings. To tackle this challenge, we propose {ConGeo}, a single- and cross-view Contrastive method for Geo-localization: it enhances robustness and consistency in feature representations to improve a model’s invariance to orientation and its resilience to {FoV} variations, by enforcing proximity between ground view variations of the same location. As a generic learning objective for cross-view geo-localization, when integrated into state-of-the-art pipelines, {ConGeo} significantly boosts the performance of three base models on four geo-localization benchmarks for diverse ground view variations and outperforms competing methods that train separate models for each ground view variation.},
	pages = {214--230},
	booktitle = {Computer Vision – {ECCV} 2024},
	publisher = {Springer Nature Switzerland},
	author = {Mi, Li and Xu, Chang and Castillo-Navarro, Javiera and Montariol, Syrielle and Yang, Wen and Bosselut, Antoine and Tuia, Devis},
	editor = {Leonardis, Aleš and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gül},
	date = {2025},
	langid = {english},
}

@article{castillo-navarro_semi-supervised_2022-1,
	title = {Semi-supervised semantic segmentation in Earth Observation: the {MiniFrance} suite, dataset analysis and multi-task network study},
	volume = {111},
	issn = {0885-6125, 1573-0565},
	url = {https://link.springer.com/10.1007/s10994-020-05943-y},
	doi = {10.1007/s10994-020-05943-y},
	shorttitle = {Semi-supervised semantic segmentation in Earth Observation},
	pages = {3125--3160},
	number = {9},
	journaltitle = {Machine Learning},
	shortjournal = {Mach Learn},
	author = {Castillo-Navarro, Javiera and Le Saux, Bertrand and Boulch, Alexandre and Audebert, Nicolas and Lefèvre, Sébastien},
	urldate = {2024-12-29},
	date = {2022-09},
	langid = {english},
	file = {PDF:/Users/javiera/Zotero/storage/TEI5C9MD/Castillo-Navarro et al. - 2022 - Semi-supervised semantic segmentation in Earth Observation the MiniFrance suite, dataset analysis a.pdf:application/pdf},
}

@inproceedings{dewez_mapping_2023,
	title = {Mapping Pb/Zn-stressed plant communities: challenges of centimetre- to millimetre-scale {UAV} sensing for training deep-learning schemes},
	url = {https://brgm.hal.science/hal-04113526},
	shorttitle = {Mapping Pb/Zn-stressed plant communities},
	abstract = {Plant interactions play a pivotal role in ecosystem functioning. In stressed ecosystems, dominant plants often mitigate environmental stress in their immediate vicinity enabling maintenance of low stress-tolerant species. In that case beneficiary plants are clustered around their benefactors. This process increases plant diversity and develops corresponding plant communities. Metals/metalloids rich environments challenge vegetation development making their remediation intricate. The {ANR}-funded {SixP} project investigates the role of facilitation in Zn/Pb contaminated sites as a potential tool for their remediation. Plant mapping is a key project component to discover clumped spatial distributions. For this, a set 24 459 plants spread over 352 m² , split in 24 distinct sampling patches were identified, located with differential {GNSS} and measured. That is 30 full man-days of effort. {UAV} acquisition could presumably ease the mapping endeavor in three ways: (i) surveying much larger surfaces, hence broadening statistical and environmental representativeness; (ii) archiving land cover state when the manual survey is performed; (iii) providing continuous explanatory variable layers such as topographic context and multispectral surface response to feed seemingly effortless deep-learning classification algorithms. This contribution means to highlight the challenges of providing accurate and trustworthy labels for training {AI} schemes. A lead/zinc mining district, in Sentein, central Pyrenees mountains (southern France) was active up until the 1960’s. Remaining toxic wastes act as stress control variable, while the vertical distribution of mining activities, over 1000 m of elevation, bring climate control on plant ecosystems. L’Avion Jaune company performed Uncrewed Aerial Vehicle ({UAV}) flights, before the growing season (march/june 2020) and at full growth (june/july 2020). Lidar was used for overview ground topography (with canopy top density ca. 50 pts/m²) and {SfM}-photogrammetry at two resolutions for land cover: 20 mm/pixel ({RGB}) before growth and 2-3 mm/pixel ({RGB}+near-infrared) at full growth season. Millimetric resolution is justified by metallic toxicity constraining dwarf plant development. Full growth mm-resolution covered 7 zones for a total of 1.34 ha (40 times the ground-truth surface), while overview cm-resolution data covered 36.6 ha, 1000 times the reference. The first lesson: differential-{GNSS} survey in mountain terrains is not trivial. No phone coverage meant no real-time corrections, nor accurate national geodesic network tie. A {GNSS} base station was freely deployed alongside a rover unit to broadcast real-time corrections. Base-to-reference-network post-processing tied surveys to {IGN}’s {RGP} network, but only with a limited accuracy (ca. 20 cm). Stationing a known benchmark with the base station would have improved accuracy. Fortunately, plant and {UAV} surveys were performed in a common spring-summer 2020 reference frame, with temporary {UAV} targets positioned by the plant-mapping teams. {GNSS} plant positions are coherent with {UAV} data to within 10-30 mm. The second lesson: producing {UAV} photogrammetric coverages at 2-3 millimeter pixels of plants is incredibly challenging. The {UAV} flew 10-m above ground to reach image resolution. But flying low strains photographic and photogrammetric processing to the limit. The {UAV} blew wind on the tiny gracile leaves and stems. Fast shutter speed of 1/1000 s counter-acted both motion blur and pixel ovalization due to {UAV} vibrations but imposed a wide aperture (f/3.5). This mitigated vignetting, refaction and diffraction issues of small apertures. Yet narrower apertures would have increased the depth-of-field, but at the cost of increasing image electronic noise to preserve shutter speed. Sensor sentivity was set at 200 {ISO}. While with these settings, plants were reasonably sharp, the wind shifted the position of mobile plant organs between photos. This challenged photogrammetric surface reconstruction and orthophotos mosaicking. Finally, {RGB} and Near-Infrared imagers were flown simultaneously, back-to-back to generate 4-band ortho-imagery. But with two points of view, even if only 8 cm apart, the base-line was sufficiently large to generate differential parallax between images, challenging orthorectification along elevated object edges. This means that {RGB} pixel signatures do not always match the underlying {NIR} signal depending on the local topography. The third lesson: training {AI} models for class (here species) segmentation requires a trustworthy training set. {GNSS}-tagged labels link a 1D position with a plant label and radius. In raster form, it is a label disk map. But the plant may not be exactly, at pixel scale, below the {GNSS} point, due to geometric imprecision. We innovated with a fuzzy edge labelling map to accounts for location uncertainty (Guiotte et al., in prep.) Finally, can trained models be transposed to different sites? {ImageNet} feature mapping clustered with t-{SNE} (Castillo-Navarro et al., 2022) maps image features to predict the chances of successful classification.},
	eventtitle = {Virtual Geoscience Conference 2023},
	author = {Dewez, Thomas {JB} and Breton, Adrien and Boisvilliers, Marie De and Bellenfant, Gaël and Houlès, Marion and Guiotte, Florent and Roux, Bruno and Castillo-Navarro, Javiera and Garcia, Guglielmo Fernandez and Lefèvre, Sébastien and Kröber, Felix and Corpetti, Thomas and Delerue, Florian},
	urldate = {2024-12-29},
	date = {2023-09-20},
	langid = {english},
	file = {Full Text PDF:/Users/javiera/Zotero/storage/JS2H227R/Dewez et al. - 2023 - Mapping PbZn-stressed plant communities challenges of centimetre- to millimetre-scale UAV sensing.pdf:application/pdf},
}

@misc{dufumier_what_2024,
	title = {What to align in multimodal contrastive learning?},
	url = {http://arxiv.org/abs/2409.07402},
	doi = {10.48550/arXiv.2409.07402},
	abstract = {Humans perceive the world through multisensory integration, blending the information of different modalities to adapt their behavior. Contrastive learning offers an appealing solution for multimodal self-supervised learning. Indeed, by considering each modality as a different view of the same entity, it learns to align features of different modalities in a shared representation space. However, this approach is intrinsically limited as it only learns shared or redundant information between modalities, while multimodal interactions can arise in other ways. In this work, we introduce {CoMM}, a Contrastive {MultiModal} learning strategy that enables the communication between modalities in a single multimodal space. Instead of imposing cross- or intra- modality constraints, we propose to align multimodal representations by maximizing the mutual information between augmented versions of these multimodal features. Our theoretical analysis shows that shared, synergistic and unique terms of information naturally emerge from this formulation, allowing us to estimate multimodal interactions beyond redundancy. We test {CoMM} both in a controlled and in a series of real-world settings: in the former, we demonstrate that {CoMM} effectively captures redundant, unique and synergistic information between modalities. In the latter, {CoMM} learns complex multimodal interactions and achieves state-of-the-art results on the six multimodal benchmarks.},
	number = {{arXiv}:2409.07402},
	publisher = {{arXiv}},
	author = {Dufumier, Benoit and Castillo-Navarro, Javiera and Tuia, Devis and Thiran, Jean-Philippe},
	urldate = {2024-12-29},
	date = {2024-09-11},
	eprinttype = {arxiv},
	eprint = {2409.07402 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/javiera/Zotero/storage/N2GXKNT9/Dufumier et al. - 2024 - What to align in multimodal contrastive learning.pdf:application/pdf;Snapshot:/Users/javiera/Zotero/storage/4E8ATHFW/2409.html:text/html},
}

@inproceedings{prado_training_2024,
	title = {Training Visual Language Models with Object Detection: Grounded Change Descriptions in Satellite Images},
	doi = {10.1109/IGARSS53475.2024.10641080},
	shorttitle = {Training Visual Language Models with Object Detection},
	pages = {2745--2749},
	author = {Prado, João and Montariol, Syrielle and Castillo-Navarro, Javiera and Tuia, Devis and Bosselut, Antoine},
	date = {2024-07-07},
}

@inproceedings{prado_training_2024-1,
	title = {Training Visual Language Models with Object Detection: Grounded Change Descriptions in Satellite Images},
	url = {https://ieeexplore.ieee.org/document/10641080?denied=},
	doi = {10.1109/IGARSS53475.2024.10641080},
	shorttitle = {Training Visual Language Models with Object Detection},
	abstract = {Recently, generalist Vision Language Models ({VLMs}) have shown exceptional progress in tasks previously dominated by specialized computer vision models. This becomes more prevalent when visual grounding capabilities, such as the ability to reason over input text and image to generate bounding boxes around objects, are required. However, how these capabilities transfer to specialized domains such as remote sensing remains understudied, despite the recent increase in specialized models for Earth observation. In this work, we evaluate how grounding visual entities – by generating bounding-box coordinates – affects {VLM} performance in satellite imagery. To this end, we create two instruction-following tasks sourced from the {xBD} dataset, describing changes due to natural disasters observed in satellite images. We fine-tune several instances of {MiniGPTv}2, an open-source {VLM} with grounding capabilities, and evaluate their performance under the "grounded" vs. "not grounded" settings. We find that generating bounding boxes to refer to visual entities increases performance in tasks related to objects in the image, but only when the number of entities in the image is limited.},
	eventtitle = {{IGARSS} 2024 - 2024 {IEEE} International Geoscience and Remote Sensing Symposium},
	pages = {2745--2749},
	booktitle = {{IGARSS} 2024 - 2024 {IEEE} International Geoscience and Remote Sensing Symposium},
	author = {Prado, João Luis and Montariol, Syrielle and Castillo-Navarro, Javiera and Tuia, Devis and Bosselut, Antoine},
	urldate = {2024-12-29},
	date = {2024-07},
	note = {{ISSN}: 2153-7003},
	keywords = {Computational modeling, Disasters, Earth Observation, Grounding, Object detection, Object Detection, Satellite images, Training, Vision-Language Models, Visualization},
	file = {IEEE Xplore Abstract Record:/Users/javiera/Zotero/storage/EKLI2HW3/10641080.html:text/html},
}

@report{zermatten_ecosystem_2024,
	title = {Ecosystem mapping with remote sensing images and ground observations},
	url = {https://meetingorganizer.copernicus.org/EGU24/EGU24-8898.html},
	number = {{EGU}24-8898},
	institution = {Copernicus Meetings},
	author = {Zermatten, Valerie and Castillo-Navarro, Javiera and Marcos, Diego and Tuia, Devis},
	urldate = {2024-12-29},
	date = {2024-03-07},
	langid = {english},
	doi = {10.5194/egusphere-egu24-8898},
	note = {Conference Name: {EGU}24},
	file = {Snapshot:/Users/javiera/Zotero/storage/IAR5E67X/EGU24-8898.html:text/html},
}

@article{zermatten_land_2024,
	title = {Land Cover Mapping From Multiple Complementary Experts Under Heavy Class Imbalance},
	volume = {17},
	issn = {2151-1535},
	url = {https://ieeexplore.ieee.org/document/10444917},
	doi = {10.1109/JSTARS.2024.3369876},
	abstract = {Deep learning has emerged as a promising avenue for automatic mapping, demonstrating high efficacy in land cover categorization through various semantic segmentation models. Nonetheless, the practical deployment of these models encounters important challenges from the imbalanced distribution of samples between the classes, a problem inherent to real-world datasets. This results in models biased towards frequent classes that perform poorly on rare classes. While existing approaches to fight class imbalance mainly focus on image classification, here we propose to address this issue for semantic segmentation with a multiple complementary experts ({MCE}) structure. Taking inspiration from ensemble models, each expert in our {MCE} specializes in certain classes and works with other experts in a complementary manner to generate robust predictions for rare classes. We compare our approach to other existing methods and also explore different logit aggregation methods, to identify the performance upper bounds and improvement directions. Our model is evaluated on a large-scale and challenging alpine land cover dataset that we make openly available. In addition, we evaluated our model on an imbalanced land cover mapping dataset, {FLAIR}, to highlight its adaptability. Overall, our {MCE} model yields notable improvement in performances on the medium and rare classes compared to baseline methods, while only slightly compromising on the overall accuracy. Despite its simplicity, the {MCE} approach stands as a practical solution for more operational semantic segmentation models, not trading off performances on rare but important classes.},
	pages = {6468--6477},
	journaltitle = {{IEEE} Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Zermatten, Valerie and Lu, Xiaolong and Castillo-Navarro, Javiera and Kellenberger, Tobias and Tuia, Devis},
	urldate = {2024-12-29},
	date = {2024},
	note = {Conference Name: {IEEE} Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	keywords = {Adaptation models, Class imbalance, land cover mapping, Land surface, multi-expert model, remote sensing, Semantic segmentation, Tail, Task analysis, Training, Visualization},
	file = {Full Text PDF:/Users/javiera/Zotero/storage/FTQYLJK6/Zermatten et al. - 2024 - Land Cover Mapping From Multiple Complementary Experts Under Heavy Class Imbalance.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/javiera/Zotero/storage/TUD98NLN/10444917.html:text/html},
}
