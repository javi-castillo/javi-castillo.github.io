---
title: What to align in multimodal contrastive learning?
authors:
- Benoit Dufumier
- Javiera Castillo-Navarro
- Devis Tuia
- Jean-Philippe Thiran
date: '2024-09-11'
publishDate: '2024-12-29T17:29:25.115210Z'
publication_types:
- manuscript
publication: '*arXiv*'
doi: 10.48550/arXiv.2409.07402
abstract: 'Humans perceive the world through multisensory integration, blending the
  information of different modalities to adapt their behavior. Contrastive learning
  offers an appealing solution for multimodal self-supervised learning. Indeed, by
  considering each modality as a different view of the same entity, it learns to align
  features of different modalities in a shared representation space. However, this
  approach is intrinsically limited as it only learns shared or redundant information
  between modalities, while multimodal interactions can arise in other ways. In this
  work, we introduce CoMM, a Contrastive MultiModal learning strategy that enables
  the communication between modalities in a single multimodal space. Instead of imposing
  cross- or intra- modality constraints, we propose to align multimodal representations
  by maximizing the mutual information between augmented versions of these multimodal
  features. Our theoretical analysis shows that shared, synergistic and unique terms
  of information naturally emerge from this formulation, allowing us to estimate multimodal
  interactions beyond redundancy. We test CoMM both in a controlled and in a series
  of real-world settings: in the former, we demonstrate that CoMM effectively captures
  redundant, unique and synergistic information between modalities. In the latter,
  CoMM learns complex multimodal interactions and achieves state-of-the-art results
  on the six multimodal benchmarks.'
tags:
- Computer Science - Artificial Intelligence
- Computer Science - Computation and Language
- Computer Science - Computer Vision and Pattern Recognition
- Computer Science - Machine Learning
links:
- name: URL
  url: http://arxiv.org/abs/2409.07402
---
