---
title: 'Learning what images cannot tell us alone'

event: MVEO workshop - BMVC 2025
event_url: https://mveo.github.io/

location: BMVC 2025. Sheffield, UK.
# address:
#   street: 450 Serra Mall
#   city: Stanford
#   region: CA
#   postcode: '94305'
#   country: United States

summary: 'Vision and language in remote sensing'
abstract: 'Earth Observation (EO) data analysis plays a major role in how we understand our planet and its dynamics. However, the models we usually rely on for this analysis often use rigid taxonomies and single-modality supervision, far from the flexible way humans perceive and interpret the world. Human understanding is inherently multimodal: we combine vision, language, and contextual knowledge to make sense of complex environments. How can we build models that move in this more “human-like” direction?
Recent advances in vision–language and multimodal learning offer a promising path. By integrating satellite imagery with natural language descriptions, ecological knowledge, or other complementary modalities, these models can capture richer semantics, transfer across datasets, and interact with users through open-ended queries. They enable remote sensing systems to go beyond fixed labels and learn more general, expressive representations of the Earth’s surface.
In this talk, we will explore recent works showing how multimodal integration can reshape remote sensing: from open-vocabulary understanding to weakly supervised ecological grounding, and toward models that capture both shared and complementary cross-modal information. These advances pave the way for more flexible mapping and deeper environmental and scientific insights.'

# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
date: '2025-11-27'
# date_end: '2030-06-01T15:00:00Z'
# all_day: false

# Schedule page publish date (NOT talk date).
# publishDate: '2017-01-01T00:00:00Z'

authors:
  - admin

tags:
- Vision and language

# Is this a featured talk? (true/false)
featured: false

# image:
#   caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/bzdhc5b3Bxs)'
#   focal_point: Right

#links:
#  - icon: twitter
#    icon_pack: fab
#    name: Follow
#    url: https://twitter.com/georgecushen
# url_code: 'https://github.com'
# url_pdf: ''
url_slides: 'https://drive.google.com/file/d/1RAjN6JFhZJ4tHNe6dDqZ9Vj_l4tzjKfs/view?usp=sharing'
# url_video: 'https://youtube.com'

# Markdown Slides (optional).
#   Associate this talk with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
# slides: ""

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
# projects:
#   - example


# {{% callout note %}}
# Click on the **Slides** button above to view the built-in slides feature.
# {{% /callout %}}

# Slides can be added in a few ways:

# - **Create** slides using Hugo Blox Builder's [_Slides_](https://docs.hugoblox.com/reference/content-types/) feature and link using `slides` parameter in the front matter of the talk file
# - **Upload** an existing slide deck to `static/` and link using `url_slides` parameter in the front matter of the talk file
# - **Embed** your slides (e.g. Google Slides) or presentation video on this page using [shortcodes](https://docs.hugoblox.com/reference/markdown/).

# Further event details, including [page elements](https://docs.hugoblox.com/reference/markdown/) such as image galleries, can be added to the body of this page.

---